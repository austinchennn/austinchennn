{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0188c0a6",
   "metadata": {},
   "source": [
    "### 1.导入一些相关的包/库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b54aad",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "import torch                        # PyTorch 的核心库，包含张量（Tensor）操作\n",
    "import torch.nn as nn               # Neural Network (神经网络) 模块，包含各种层（如全连接层、卷积层）\n",
    "import torch.nn.functional as F     # 函数式接口，包含激活函数、损失函数等无参数的计算\n",
    "from torch.utils.data import Dataset    # 数据集基类，用于定义\"怎么读单个数据\"\n",
    "from torch.utils.data import DataLoader # 数据加载器，用于定义\"怎么把数据打包(batch)喂给模型\"\n",
    "from dataclasses import dataclass   # Python 标准库，用于快速创建只包含数据的类（常用作配置参数）\n",
    "import math                         # Python 标准库，包含数学运算函数\n",
    "# 设置 CPU 生成随机数的种子，保证实验结果可复现\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012d840",
   "metadata": {},
   "source": [
    "### 2.定义一些GPT参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc47a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. 记忆力 (Context Window)\n",
    "    # -----------------------------------------------------------\n",
    "    block_size: int = 512\n",
    "    # 含义：上下文窗口大小 / 序列最大长度\n",
    "    # 通俗理解：模型一眼能看多少个字。\n",
    "    # 详解：模型在预测下一个字时，最多只能回头看前 512 个字。\n",
    "    #      超过这个长度，前面的内容它就\"忘\"了。\n",
    "    #      (比如你在写第 513 个字时，它已经看不见第 1 个字了)。\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. 学习速度/并行能力 (Parallelism)\n",
    "    # -----------------------------------------------------------\n",
    "    batch_size: int = 12\n",
    "    # 含义：批次大小\n",
    "    # 通俗理解：老师一次批改几本作业。\n",
    "    # 详解：模型训练不是一句话一句话学的，而是把 12 句话捆在一起，\n",
    "    #      并行地喂给 GPU 计算。这个数字越大，训练越快（但也越吃显存）。\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3. 大脑深度 (Depth)\n",
    "    # -----------------------------------------------------------\n",
    "    n_layer: int = 12\n",
    "    # 含义：Transformer Block 的层数\n",
    "    # 通俗理解：模型有多少层\"过滤器\"或者\"专家\"。\n",
    "    # 详解：数据进入模型后，要经过 12 道工序的处理。\n",
    "    #      层数越深，模型逻辑推理能力越强，能理解更抽象的概念。\n",
    "    #      (GPT-2 Small 就是 12 层)。\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. 思考角度 (Attention Heads)\n",
    "    # -----------------------------------------------------------\n",
    "    n_head: int = 12\n",
    "    # 含义：多头注意力的头数\n",
    "    # 通俗理解：模型在读一句话时，有多少个\"心眼\"同时在看。\n",
    "    # 详解：比如读\"苹果\"这个词：\n",
    "    #      第1个头关注它的颜色，第2个头关注它的形状，第3个头关注它是水果...\n",
    "    #      12个头就代表它能从 12 个不同的角度去理解词与词之间的关系。\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5. 词汇理解力 (Embedding Dimension)\n",
    "    # -----------------------------------------------------------\n",
    "    n_embd: int = 768\n",
    "    # 含义：嵌入维度 / 隐藏层大小\n",
    "    # 通俗理解：用来描述一个词的\"特征向量\"有多长。\n",
    "    # 详解：在计算机眼里，\"猫\"这个字不是汉字，而是一串数字。\n",
    "    #      这里规定用 768 个数字来描述\"猫\"。\n",
    "    #      数字越多（维度越高），能包含的信息就越丰富，描述得越精准。\n",
    "    #      (注：通常 n_embd 除以 n_head 必须是整数，这里 768/12 = 64)。\n",
    "    \n",
    "    hideen_dim : int = n_embd\n",
    "    # 含义：前馈神经网络的隐藏层维度\n",
    "    # 通俗理解：模型在每个位置上\"思考\"时，能\"思考\"出多少个可能的答案。\n",
    "    # 详解：在 Transformer 的每个位置上，都有一个小型的前馈神经网络。\n",
    "    #      这个网络的隐藏层维度是 768，意味着它能\"思考\"出 768 个不同的可能性。\n",
    "    #      维度越高，模型的表达能力越强，但计算量也越大。\n",
    "    # 隐藏层是什么\n",
    "    # 隐藏层是神经网络中的一个概念，位于输入层和输出层之间。\n",
    "    # 它接收前一层的所有输入数据，并通过一系列的数学运算（如加权求和、激活函数等）处理这些数据，然后产生下一层的输入。\n",
    "    # 在Transformer模型中，每个位置的前馈神经网络的隐藏层维度通常与嵌入维度相同，即768维。这有助于保持信息的一致性和模型的表达能力。\n",
    "\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 6. 防作弊机制 (Regularization)\n",
    "    # -----------------------------------------------------------\n",
    "    dropout: float = 0.1\n",
    "    # 含义：丢弃率 (10%)\n",
    "    # 通俗理解：故意让模型\"失忆\"或者\"脑神经断连\"一下。\n",
    "    # 详解：在训练时，随机把 10% 的神经元关掉（置为0），不让它们工作。\n",
    "    #      为什么要这么做？为了防止模型\"死记硬背\"（过拟合）。\n",
    "    #      这逼迫模型必须学会举一反三，而不是只靠某几个神经元记答案。\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 7. 每个头的视野 (Head Size)\n",
    "    # -----------------------------------------------------------\n",
    "    head_size: int = n_embd // n_head\n",
    "    # 含义：每个注意力头的维度\n",
    "    # 通俗理解：每个\"心眼\"能看的信息量。\n",
    "    # 注意力头会做什么操作，怎么样用到head_size\n",
    "    # 注意力头会根据输入的词向量，计算出每个词与其他词之间的相关性（注意力分数）。\n",
    "    # 然后，它会根据这些分数，动态地调整每个词的表示，使得模型能够更好地理解词与词之间的关系。\n",
    "    # head_size 决定了每个注意力头在计算注意力分数时所使用的向量维度。\n",
    "    # head_size 是一个词一个词过的吗？\n",
    "    # head_size 不是一个词一个词过的，而是每个注意力头在处理整个输入序列时使用的向量维度。\n",
    "    # 输入序列有本质是什么\n",
    "    # 输入序列本质上是一个由多个词组成的列表，每个词都被表示为一个向量（词嵌入）。\n",
    "    # 详解：因为总的嵌入维度是 768，而注意力头有 12 个，\n",
    "    #      所以每个头分到的维度就是 768/12=64。\n",
    "    # 为什么是n_embd // n_head\n",
    "    # 因为每个注意力头需要均分总的嵌入维度。\n",
    "    # 数学本质：\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 8. 词汇表大小\n",
    "    # -----------------------------------------------------------\n",
    "    #gpt2官方的tokenizer\n",
    "    vocab_size: int = 50257\n",
    "    # 含义：词汇表大小\n",
    "    # 通俗理解：模型能认识多少个不同的词/符号。\n",
    "    # 详解：GPT-2 使用的 BPE 分词器一共包含 50257 个不同的 token。\n",
    "    #      这些 token 包括常见的汉字、字母、标点符号，甚至一些特殊符号。    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c6dbe",
   "metadata": {},
   "source": [
    "## GPT的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class singleHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # 生成 Key 矩阵\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        # key是什么？\n",
    "        # 在计算注意力分数时，模型会将输入的嵌入向量通过线性变换生成 Key 矩阵。\n",
    "        # 这个矩阵帮助模型理解输入序列中各个位置的信息，从而决定在计算注意力时应该关注哪些部分。\n",
    "        # key用来做什么？\n",
    "        # Key 矩阵用于计算注意力分数，帮助模型确定在处理当前输入时，应该关注输入序列中的哪些位置。\n",
    "        # 通过与 Query 矩阵进行点积运算，模型可以计算出每个位置的重要性，从而动态地调整对不同位置的关注度。\n",
    "        # nn.Linear(config.n_embd, config.head_size) 的含义\n",
    "        # 这表示一个线性变换层，将输入的嵌入向量从 config.n_embd 维度映射到 config.head_size 维度。\n",
    "        # config.n_embd: 嵌入维度，表示每个词的特征向量长度。\n",
    "        # config.head_size: 每个注意力头的维度，表示该头能够处理的信息量。\n",
    "        # 通过这个线性层，模型可以学习到如何将输入的高维信息压缩或转换为适合该注意力头处理的低维表示\n",
    "        # 目的：为了让模型能够有效地计算注意力分数，从而更好地理解输入序列中的信息分布。\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "        #query是什么？\n",
    "        # Query 矩阵是注意力机制中的另一个关键组成部分。\n",
    "        # 它通过线性变换将输入的嵌入向量转换为 Query 矩阵。\n",
    "        # query用来做什么？\n",
    "        # Query 矩阵用于与 Key 矩阵进行点积运算，以计算注意力分数。\n",
    "        # 这些分数反映了当前输入与输入序列中各个位置之间的相关性，帮助模型决定应该关注哪些部分的信息。\n",
    "\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        # nn.Linear(config.n_embd, config.head_size) 的返回的是Callable Object，就是一个函数\n",
    "\n",
    "\n",
    "        # attention_mask的新写法：通过register_buffer注册\n",
    "        # 因为不用计算梯度\n",
    "        # mask的作用：屏蔽掉未来的信息，只关注过去的信息\n",
    "        # 2. 这个 Mask 是用来做什么的？\n",
    "        # 核心目的：防止“剧透” (Prevent Peeking / Causal Masking)\n",
    "\n",
    "        # 在训练 GPT 这种生成式模型时，我们是把整句话一次性喂进去的。比如句子是 \"I love AI\"。 如果不加限制，Self-Attention 机制会让每个词都能看到所有其他的词。\n",
    "\n",
    "        # 当模型在处理 \"I\" 的时候，它能看到后面的 \"love\" 和 \"AI\"。\n",
    "\n",
    "        # 这就作弊了！因为在预测时，模型说完 \"I\" 之后，是不应该知道后面是 \"love\" 的。\n",
    "        \n",
    "\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            #tril是下三角矩阵\n",
    "            # block_size 是文本的最大长度\n",
    "\n",
    "            # 保留主对角线及以下的数值，把上面的全部变成 0\n",
    "            torch.tril(\n",
    "                # 创建一个全 1 的矩阵\n",
    "                torch.ones((config.block_size, config.block_size))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #dropout是什么: 随机失忆\n",
    "        # 为什么要有dropout: 解决过拟合\n",
    "        # Dropout的数学底层原理\n",
    "        # Dropout 是一种正则化技术，通过在训练过程中随机\"丢弃\"（置为零）一部分神经元的输出，来防止模型过拟合。\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        # batch_size: 批次大小\n",
    "        # seq_len: 序列长度\n",
    "        # hidden_dim: 隐藏层维度 (嵌入维度)\n",
    "        \n",
    "        # 1. 生成 Query, Key, Value\n",
    "        # [Batch, Seq, Dim] -> [Batch, Seq, Head_Size]\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        # PyTorch 的矩阵乘法 (torch.matmul 或 @) 极其“死板”，它只认最后两个维度做乘法\n",
    "        # 也就是把 [Batch, Seq, Head_Size] -> [Batch, Head_Size, Seq]\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        # -2 和 -1 分别表示倒数第二个和倒数第一个维度\n",
    "        # 就是倒数第二个和倒数第一个维度转置吗？\n",
    "        # weights 的形状是 (batch_size, seq_len, seq_len)\n",
    "        weight = weight.masked_fill(\n",
    "            # [:seq_len, :seq_len] 是因为实际的序列长度可能小于 block_size\n",
    "            # 意为着只取前 seq_len 行和前 seq_len 列，其余位置填充为 -inf\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0, \n",
    "            float('-inf')\n",
    "            )\n",
    "\n",
    "        #记得weight除以sqrt(d_k)\n",
    "        # 为什么要除以这个 8？（为什么要缩放？）\n",
    "        #这是为了救 Softmax 一命，防止梯度消失。\n",
    "        weight = weight / math.sqrt(self.head_size)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        #Attention Dropout要放在weight权重化前面\n",
    "        weight = self.dropout(weight)\n",
    "        output = weight @ v  # [Batch, Seq, Head_Size]\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd404c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094e3963",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
